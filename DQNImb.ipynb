{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQNImb.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vineetjoshi253/Using-Deep-Reinforcement-Learning-For-Imbalanced-Data-Classification/blob/main/DQNImb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5MEVx_P93Ee"
      },
      "source": [
        "import numpy as np\r\n",
        "import torch.nn as nn\r\n",
        "import torch\r\n",
        "from collections import deque\r\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imVz4tf0-N8B"
      },
      "source": [
        "#Load Data\r\n",
        "X_train = np.load('/content/79_40percent_undersampled_train.npy')\r\n",
        "Y_train = np.load('/content/79_40percent_undersampled_train_labels.npy')\r\n",
        "X_Val = np.load('/content/79_40percent_undersampled_validation.npy')\r\n",
        "Y_Val = np.load('/content/79_40percent_undersampled_validation_labels.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L2yjcVsAeeh"
      },
      "source": [
        "#### Setting Up The Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1sNOHeuA_1F",
        "outputId": "55ce5910-76ed-4791-b8fb-ae88f103e499"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7496, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynocT0AzAeMK"
      },
      "source": [
        "class Environment:\r\n",
        "  def __init__(self,X_train,Y_train,imbratio):\r\n",
        "    #Saving The Parameters\r\n",
        "    self.X = X_train.reshape((X_train.shape[0],1,28, 28))\r\n",
        "    self.Y = Y_train\r\n",
        "    self.start_state = 0\r\n",
        "    self.action_space = [0,1]\r\n",
        "    self.input_shape = (28,28)\r\n",
        "    self.imbalance_ratio = imbratio\r\n",
        "    self.ind = 0\r\n",
        "\r\n",
        "\r\n",
        "  def reset(self):\r\n",
        "    #Shuffling Data At Each Episode\r\n",
        "    Ind = [i for i in range(Self.X.shape[0])]\r\n",
        "    random.shuffle(Ind)\r\n",
        "    self.Y = self.Y[Ind]\r\n",
        "    self.X = self.X[Ind]\r\n",
        "    self.ind = 0\r\n",
        "    return self.X[self.ind]\r\n",
        "    \r\n",
        "\r\n",
        "  def step(self,state,action):\r\n",
        "    self.ind += 1\r\n",
        "    #Taking A Step In The Episode\r\n",
        "\r\n",
        "    #If Agent Is Of Positive (Minority) Class\r\n",
        "    if self.Y[self.ind-1] == 1:\r\n",
        "      #Correct Classification\r\n",
        "      if action == self.Y[self.ind-1]:\r\n",
        "         if self.ind == len(self.Y):\r\n",
        "           return self.X[0],1,True\r\n",
        "         else:\r\n",
        "           return self.X[self.ind],1,False\r\n",
        "\r\n",
        "      else:\r\n",
        "        #Incorrect Classification\r\n",
        "        if self.ind == len(self.Y):\r\n",
        "          return self.X[0],-1,True\r\n",
        "        else:\r\n",
        "          return self.X[self.ind],-1,True\r\n",
        "    #If state is of negative class\r\n",
        "    elif self.Y[self.ind-1] == 0:\r\n",
        "      #Incorrect Classification\r\n",
        "      if action != self.Y[self.ind-1]:\r\n",
        "        \r\n",
        "        if self.ind == len(self.Y):\r\n",
        "           return self.X[0],-1 * self.imbalance_ratio,True\r\n",
        "        else:\r\n",
        "           return self.X[self.ind],-1 * self.imbalance_ratio,False\r\n",
        "      \r\n",
        "      else:\r\n",
        "        #Correct Classification\r\n",
        "        if self.ind == len(self.Y):\r\n",
        "           return self.X[0],self.imbalance_ratio,True\r\n",
        "        else:\r\n",
        "           return self.X[self.ind],self.imbalance_ratio,False\r\n",
        "\r\n",
        "imbalance_ratio = 0.4\r\n",
        "env = Environment(X_train,Y_train,imbalance_ratio)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "333xLidwBP3P"
      },
      "source": [
        "### Setting Up The Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlIQUB5NAdVR"
      },
      "source": [
        "#Discount Factor\r\n",
        "gamma = 0.1\r\n",
        "#Batch Size For Training The Networks\r\n",
        "batch_size = 64\r\n",
        "#Maximum Number Of Episodes\r\n",
        "episodes = 500\r\n",
        "\r\n",
        "#Maximum steps in each episode.\r\n",
        "episode_steps = 1000\r\n",
        "#Starting epsilon value.\r\n",
        "start_epsilon = 1\r\n",
        "#End epsilon value\r\n",
        "end_epsilon = 0.001\r\n",
        "decay_stop = 50\r\n",
        "\r\n",
        "#Setting the exploration rate. \r\n",
        "exploration_rate = start_epsilon\r\n",
        "epsilon_delta = 0.0001\r\n",
        "replay_size = 50000\r\n",
        "target_update_freq = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIGSBQNLCpZH"
      },
      "source": [
        "F1 = []\r\n",
        "Rewards = []\r\n",
        "C_Rewards = []\r\n",
        "\r\n",
        "#Deep Q Network Class\r\n",
        "class Deep_QNetwork:\r\n",
        "    def __init__(self, env, gamma, batch_size, memory_max_size, episodes, exploration_rate, epsilon_delta, decay_stop, replay_size, target_update_freq, episode_steps):\r\n",
        "        #Set up the hyperparameters.\r\n",
        "        self.env = env\r\n",
        "        self.gamma = gamma\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.memory_max_size = memory_max_size\r\n",
        "        self.episodes = episodes\r\n",
        "        self.exploration_rate = exploration_rate\r\n",
        "        self.epsilon_delta = epsilon_delta\r\n",
        "        self.decay_stop = decay_stop\r\n",
        "        self.replay_size = replay_size\r\n",
        "        self.target_update_freq = target_update_freq\r\n",
        "        self.episode_steps = episode_steps\r\n",
        "        self.steps = 0\r\n",
        "        self.mse_loss = nn.MSELoss()\r\n",
        "\r\n",
        "        #Create replay memory.\r\n",
        "        self.replay_memory = deque(maxlen = replay_size)\r\n",
        "\r\n",
        "        #Create the Q and the target networks.\r\n",
        "        self.QNetwork = self.Create_QNetwork().to('cuda')\r\n",
        "        self.target_network = self.Create_QNetwork().to('cuda')\r\n",
        "    \r\n",
        "    #Function to create the sequential models. \r\n",
        "    def Create_QNetwork(self):\r\n",
        "        model = nn.Sequential(nn.Conv2d(1,32,5),nn.ReLU(),nn.MaxPool2d(2),nn.Conv2d(32,32,5),nn.ReLU(),nn.MaxPool2d(2),nn.Flatten(),nn.Linear(512,256),nn.ReLU(),nn.Linear(256,2),nn.Softmax(1))\r\n",
        "        return model\r\n",
        "\r\n",
        "    #Function to predict the state-action values based on the given Q Model. \r\n",
        "    def predict(self, model, state):\r\n",
        "        return model(state.cuda())\r\n",
        "\r\n",
        "    #Function to find action based on the current state and E-Greedy policy.\r\n",
        "    def find_action(self, state):\r\n",
        "        #Exploration\r\n",
        "        if(np.random.rand() < self.exploration_rate):\r\n",
        "            Action = np.random.randint(0, len(self.env.action_space))\r\n",
        "        #Exploitation \r\n",
        "        else:\r\n",
        "            Q_Values = self.predict(self.QNetwork, state)\r\n",
        "            Action = Q_Values.argmax().item()\r\n",
        "        return Action\r\n",
        "\r\n",
        "    #Function to predict class based on the QNetwork. \r\n",
        "    def predict_class(self, state):\r\n",
        "      Q_Values = self.predict(self.QNetwork, state)\r\n",
        "      Action = Q_Values.argmax().item()\r\n",
        "      return Action\r\n",
        "    \r\n",
        "    #Append current observations to the replay memory.\r\n",
        "    def append_batch(self, state, action, reward, next_state, done):\r\n",
        "        self.replay_memory.append((state, action, reward, next_state, done))\r\n",
        "\r\n",
        "    #Sample current batch from the replay Memory\r\n",
        "    def generate_batch(self):\r\n",
        "        if(len(self.replay_memory) < batch_size):\r\n",
        "            size = len(self.replay_memory)\r\n",
        "        else:\r\n",
        "            size = self.batch_size\r\n",
        "        batch = random.sample(self.replay_memory, k = size)\r\n",
        "        return batch\r\n",
        "    \r\n",
        "    #Update target network.\r\n",
        "    def Update_Target(self):\r\n",
        "        self.target_network.load_state_dict(self.QNetwork.state_dict())\r\n",
        "\r\n",
        "    def replay(self):\r\n",
        "        #Batch of experience is renadomly sampled from the replay memory which stores the sequence of state, action and rewards\r\n",
        "        mini_batch = self.generate_batch()\r\n",
        "        batch_size = len(mini_batch)\r\n",
        "\r\n",
        "        #Creating a dictionary for all the components(state, next state, reward, done, action) of the episodes of the batch generated\r\n",
        "        #Each component of the dictionary is intialized with a tensor filled with the scalar value 0, with the shape defined by the argument passed\r\n",
        "        batch_dict = {'states': torch.zeros(batch_size,1,28,28),\r\n",
        "                      'next_states': torch.zeros(batch_size,1,28,28),\r\n",
        "                      'rewards' : torch.zeros(batch_size),\r\n",
        "                      'dones': torch.zeros(batch_size),\r\n",
        "                      'actions': torch.zeros(batch_size)}\r\n",
        "\r\n",
        "        #Storing the values of the batch generated in the above defined dictionary\r\n",
        "        index = 0\r\n",
        "        for state, action, reward, next_state, done in mini_batch:\r\n",
        "          batch_dict['states'][index] = state\r\n",
        "          batch_dict['next_states'][index] = next_state\r\n",
        "          batch_dict['rewards'][index] = reward\r\n",
        "          batch_dict['dones'][index] = done \r\n",
        "          batch_dict['actions'][index] = action\r\n",
        "          index += 1\r\n",
        "        \r\n",
        "        for key in batch_dict:\r\n",
        "          batch_dict[key] = batch_dict[key].float()\r\n",
        "\r\n",
        "        #Use the target model for computing the next Q values       \r\n",
        "        next_state_qvalues = self.predict(self.target_network, batch_dict['next_states']).detach()\r\n",
        "\r\n",
        "        #find the maximum value of the next Q values\r\n",
        "        next_state_qvalues_max = next_state_qvalues.max(axis=1)[0]\r\n",
        "\r\n",
        "        #If the episode terminates, we set the target value as reward, else (reward + gamma * max_qvalue(next state))\r\n",
        "        mask = 1 - batch_dict['dones']\r\n",
        "        masked_q_values = torch.mul(mask.to('cuda'), next_state_qvalues_max)\r\n",
        "\r\n",
        "        #finding the target value (reward + gamma * max_qvalue(next state))\r\n",
        "        target = batch_dict['rewards'].to('cuda') + gamma * masked_q_values\r\n",
        "\r\n",
        "        #Use the Q network for computing the qvalues of the current states\r\n",
        "        q_values = self.predict(self.QNetwork, batch_dict['states'])\r\n",
        "        final_qvalues = q_values.gather(dim=1, index=batch_dict['actions'].unsqueeze(dim=1).long().to('cuda')).squeeze(dim=1)\r\n",
        "\r\n",
        "        #Calculate the mean squared error\r\n",
        "        loss = self.mse_loss(final_qvalues, target)\r\n",
        "        return loss\r\n",
        "\r\n",
        "    #Calcualte the F1 Score Based On The Current Model on Testing Data\r\n",
        "    def validation_F1(self,X_test,Y_test):\r\n",
        "      Y_pred = []\r\n",
        "      for i in range(X_test.shape[0]):\r\n",
        "        Inpt = torch.from_numpy(X_test[i].reshape(3,32,32)).float().unsqueeze(dim=0)\r\n",
        "        a = DQN.predict_class(Inpt)\r\n",
        "        Y_pred.append(a)\r\n",
        "      f1 = f1_score(Y_test,Y_pred)\r\n",
        "      F1.append(f1)\r\n",
        "      return f1\r\n",
        "\r\n",
        "    #Save Models\r\n",
        "    def save_model(self):\r\n",
        "      torch.save(self.QNetwork.state_dict(),'/content/drive/MyDrive/RL Project/Models/CIFAR 0.4/DQN_QNetwork_model_16_40per.h5')\r\n",
        "      torch.save(self.target_network.state_dict(),'/content/drive/MyDrive/RL Project/Models/CIFAR 0.4/DQN_Target_model_16_40per.h5')\r\n",
        "      print('Model Saved')\r\n",
        "    \r\n",
        "    def Deep_QModel(self,X_test,Y_test):\r\n",
        "        \r\n",
        "        Episode_Count = 0\r\n",
        "        step_count = 0\r\n",
        "        optimizer = torch.optim.Adam(self.QNetwork.parameters(), lr = 0.00025) \r\n",
        "        Total_reward_so_far = 0\r\n",
        "        \r\n",
        "        for episode in tq.tqdm(range(self.episodes)):\r\n",
        "            #Get current state of the environment. \r\n",
        "            state = env.reset()\r\n",
        "            state = torch.from_numpy(state).float().unsqueeze(dim=0)\r\n",
        "\r\n",
        "            done = False\r\n",
        "            total_reward = 0\r\n",
        "            total_loss = 0\r\n",
        "\r\n",
        "            #Iterate till maximum number of steps. \r\n",
        "            for step in range(0, self.episode_steps):\r\n",
        "                self.steps+=1\r\n",
        "                step_count += 1\r\n",
        "                #Get Action\r\n",
        "                action = self.find_action(state)\r\n",
        "                \r\n",
        "                #Get Next State & Reward\r\n",
        "                next_state, reward, done = self.env.step(state,action)\r\n",
        "                next_state = torch.from_numpy(next_state).float().unsqueeze(dim=0)\r\n",
        "\r\n",
        "                total_reward += reward\r\n",
        "                Total_reward_so_far+= reward\r\n",
        "\r\n",
        "                #Append the observations to replay memory.\r\n",
        "                self.append_batch(state, action, reward, next_state, done)\r\n",
        "\r\n",
        "                #Update the current state.\r\n",
        "                state = next_state\r\n",
        "\r\n",
        "                #Calculate Loss and optimize the Q Network\r\n",
        "                if(len(self.replay_memory) > self.batch_size):\r\n",
        "                    loss = self.replay()\r\n",
        "                    total_loss += loss.item()\r\n",
        "                    loss.to('cuda')\r\n",
        "                    optimizer.zero_grad()\r\n",
        "                    loss.backward()\r\n",
        "                    optimizer.step()\r\n",
        "\r\n",
        "                #Update the exploration rate\r\n",
        "                if self.exploration_rate > end_epsilon:\r\n",
        "                    self.exploration_rate -= self.epsilon_delta\r\n",
        "\r\n",
        "                #Update the target network.\r\n",
        "                if (step_count % self.target_update_freq) == 0:\r\n",
        "                    self.Update_Target()\r\n",
        "\r\n",
        "                if done:\r\n",
        "                    if episode%10 == 0:\r\n",
        "                      f1 = self.validation_F1(X_test,Y_test)\r\n",
        "                      print('Episode: ',episode,'Current F1: ',f1)\r\n",
        "                      self.save_model()\r\n",
        "                      \r\n",
        "                    Episode_Count = Episode_Count + 1\r\n",
        "                    break\r\n",
        "\r\n",
        "            Rewards.append(total_reward)\r\n",
        "            C_Rewards.append(Total_reward_so_far)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QS3WpRAMG-AS"
      },
      "source": [
        "### Running The Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FmBK6nlGXvO"
      },
      "source": [
        "DQN = Deep_QNetwork(env, gamma, batch_size, replay_size, episodes, exploration_rate, epsilon_delta, decay_stop, replay_size, target_update_freq, episode_steps)\r\n",
        "Rewards = DQN.Deep_QModel(X_Val,Y_Val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyOI5bW4Qg1b"
      },
      "source": [
        "### Generating Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_MZc7aanL_I"
      },
      "source": [
        "#Generate the predictions\r\n",
        "Y_pred = []\r\n",
        "for i in range(X_val.shape[0]):\r\n",
        "  Inpt = torch.from_numpy(X_val[i].reshape(3,32,32)).float().unsqueeze(dim=0)\r\n",
        "  a = DQN.predict_class(Inpt)\r\n",
        "  Y_pred.append(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsWB9akFMpoV"
      },
      "source": [
        "print('F1 Score: ',f1_score(Y_val,Y_pred))\r\n",
        "print('Accuracy: ',accuracy_score(Y_val,Y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}