{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "REINFORCEimb.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMhnNyI7oHryYQZ+pG+sB/H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vineetjoshi253/Using-Deep-Reinforcement-Learning-For-Imbalanced-Data-Classification/blob/main/REINFORCEimb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5MEVx_P93Ee"
      },
      "source": [
        "import numpy as np\r\n",
        "import random\r\n",
        "import tensorflow as tf\r\n",
        "from keras.models import Sequential, Model\r\n",
        "from keras.layers import Dense, Dropout, Input,Flatten\r\n",
        "from keras.layers.merge import Add, Multiply\r\n",
        "from keras.optimizers import Adam\r\n",
        "import keras.backend as K\r\n",
        "import tqdm.notebook as tq\r\n",
        "from keras.layers import Conv2D\r\n",
        "import tqdm.notebook as tq\r\n",
        "from keras.layers import MaxPooling2D\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.image as mpimg\r\n",
        "from collections import deque\r\n",
        "from sklearn.metrics import f1_score\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from tensorflow.python.keras.models import load_model\r\n",
        "\r\n",
        "import logging\r\n",
        "import os\r\n",
        "\r\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # FATAL\r\n",
        "logging.getLogger('tensorflow').setLevel(logging.FATAL)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imVz4tf0-N8B"
      },
      "source": [
        "#Load Data\r\n",
        "X_train = np.load('/content/79_40percent_undersampled_train.npy')\r\n",
        "Y_train = np.load('/content/79_40percent_undersampled_train_labels.npy')\r\n",
        "X_Val = np.load('/content/79_40percent_undersampled_validation.npy')\r\n",
        "Y_Val = np.load('/content/79_40percent_undersampled_validation_labels.npy')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L2yjcVsAeeh"
      },
      "source": [
        "#### Setting Up The Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1sNOHeuA_1F",
        "outputId": "dd7cf5d1-f506-4b74-9d11-af24981c0969"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7496, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynocT0AzAeMK"
      },
      "source": [
        "class Environment:\r\n",
        "  def __init__(self,X_train,Y_train,imbratio):\r\n",
        "    #Saving The Parameters\r\n",
        "    self.X = X_train.reshape((X_train.shape[0],28, 28,1))\r\n",
        "    self.Y = Y_train\r\n",
        "    self.start_state = 0\r\n",
        "    self.action_space = [0,1]\r\n",
        "    self.input_shape = (28,28)\r\n",
        "    self.imbalance_ratio = imbratio\r\n",
        "    self.ind = 0\r\n",
        "\r\n",
        "\r\n",
        "  def reset(self):\r\n",
        "    #Shuffling Data At Each Episode\r\n",
        "    Ind = [i for i in range(self.X.shape[0])]\r\n",
        "    random.shuffle(Ind)\r\n",
        "    self.Y = self.Y[Ind]\r\n",
        "    self.X = self.X[Ind]\r\n",
        "    self.ind = 0\r\n",
        "    return self.X[self.ind]\r\n",
        "    \r\n",
        "\r\n",
        "  def step(self,state,action):\r\n",
        "    self.ind += 1\r\n",
        "    #Taking A Step In The Episode\r\n",
        "\r\n",
        "    #If Agent Is Of Positive (Minority) Class\r\n",
        "    if self.Y[self.ind-1] == 1:\r\n",
        "      #Correct Classification\r\n",
        "      if action == self.Y[self.ind-1]:\r\n",
        "         if self.ind == len(self.Y):\r\n",
        "           return self.X[0],1,True\r\n",
        "         else:\r\n",
        "           return self.X[self.ind],1,False\r\n",
        "\r\n",
        "      else:\r\n",
        "        #Incorrect Classification\r\n",
        "        if self.ind == len(self.Y):\r\n",
        "          return self.X[0],-1,True\r\n",
        "        else:\r\n",
        "          return self.X[self.ind],-1,True\r\n",
        "    #If agent is of positive class\r\n",
        "    elif self.Y[self.ind-1] == 0:\r\n",
        "      #Incorrect Classification\r\n",
        "      if action != self.Y[self.ind-1]:\r\n",
        "        \r\n",
        "        if self.ind == len(self.Y):\r\n",
        "           return self.X[0],-1 * self.imbalance_ratio,True\r\n",
        "        else:\r\n",
        "           return self.X[self.ind],-1 * self.imbalance_ratio,False\r\n",
        "      \r\n",
        "      else:\r\n",
        "        #Correct Classification\r\n",
        "        if self.ind == len(self.Y):\r\n",
        "           return self.X[0],self.imbalance_ratio,True\r\n",
        "        else:\r\n",
        "           return self.X[self.ind],self.imbalance_ratio,False\r\n",
        "\r\n",
        "imbalance_ratio = 0.4\r\n",
        "env = Environment(X_train,Y_train,imbalance_ratio)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "333xLidwBP3P"
      },
      "source": [
        "### Setting Up The Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADd5RYdrI0g_"
      },
      "source": [
        "gamma = 0.1\r\n",
        "alpha= 1e-4\r\n",
        "learning_rate= 0.00025"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKk2HMH3IZ3N"
      },
      "source": [
        "#Creating the class for the reinforce learning agent.\r\n",
        "class REINFORCE:\r\n",
        "  def __init__(self,env,gamma,alpha,learning_rate,path = None):\r\n",
        "    self.env=env \r\n",
        "    self.state_shape=env.input_shape\r\n",
        "    self.action_shape= len(env.action_space)\r\n",
        "    self.gamma= gamma\r\n",
        "    self.alpha= 1e-4\r\n",
        "    self.learning_rate= learning_rate\r\n",
        "    \r\n",
        "    #Load the policy prediction model. \r\n",
        "    if not path:\r\n",
        "      self.model=self._create_model() \r\n",
        "    else:\r\n",
        "      self.model=self.load_model(path) \r\n",
        "\r\n",
        "    #Saving the data.\r\n",
        "    self.states=[]\r\n",
        "    self.gradients=[] \r\n",
        "    self.rewards=[]\r\n",
        "    self.probs=[]\r\n",
        "    self.discounted_rewards=[]\r\n",
        "    self.total_rewards=[]\r\n",
        "    self.f1 = []\r\n",
        "  \r\n",
        "  #Create the model. \r\n",
        "  def _create_model(self):\r\n",
        "    model=Sequential()\r\n",
        "    model.add(Conv2D(32, (5, 5), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28,1)))\r\n",
        "    model.add(MaxPooling2D((2, 2)))\r\n",
        "    model.add(Conv2D(32, (5, 5), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28,1)))\r\n",
        "    model.add(MaxPooling2D((2, 2)))\r\n",
        "    model.add(Flatten())\r\n",
        "    model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\r\n",
        "    model.add(Dense(10, activation='softmax'))\r\n",
        "    model.add(Dense(self.action_shape, activation=\"softmax\"))\r\n",
        "    model.compile(loss=\"categorical_crossentropy\",optimizer=Adam(lr=self.learning_rate))\r\n",
        "    return model\r\n",
        "\r\n",
        "  #Encode Actions\r\n",
        "  def hot_encode_action(self, action):\r\n",
        "    action_encoded=np.zeros(self.action_shape, np.float32)\r\n",
        "    action_encoded[action]=1\r\n",
        "    return action_encoded\r\n",
        "  \r\n",
        "  #Update the memory with current observations.\r\n",
        "  def update_memory(self, state, action, action_prob, reward):\r\n",
        "    encoded_action=self.hot_encode_action(action)\r\n",
        "    self.gradients.append(encoded_action-action_prob)\r\n",
        "    self.states.append(state)\r\n",
        "    self.rewards.append(reward)\r\n",
        "    self.probs.append(action_prob)\r\n",
        "\r\n",
        "  #Get action\r\n",
        "  def get_action(self, state):\r\n",
        "    #Current State \r\n",
        "    state=state.reshape((1,28,28,1))\r\n",
        "\r\n",
        "    #Probability distribibution \r\n",
        "    action_probability_distribution=self.model.predict(state).flatten()\r\n",
        "    action_probability_distribution/=np.sum(action_probability_distribution)\r\n",
        "\r\n",
        "    #Select Action\r\n",
        "    action=np.random.choice(self.action_shape,1,p=action_probability_distribution)[0]\r\n",
        "\r\n",
        "    return action, action_probability_distribution\r\n",
        "\r\n",
        "  #Function to generate the discounted reward.\r\n",
        "  def get_discounted_rewards(self, rewards): \r\n",
        "    discounted_rewards=[]\r\n",
        "    cumulative_total_return=0\r\n",
        "\r\n",
        "    for reward in rewards[::-1]:      \r\n",
        "      cumulative_total_return=(cumulative_total_return*self.gamma)+reward\r\n",
        "      discounted_rewards.insert(0, cumulative_total_return)\r\n",
        "\r\n",
        "    mean_rewards=np.mean(discounted_rewards)\r\n",
        "    std_rewards=np.std(discounted_rewards)\r\n",
        "    norm_discounted_rewards=(discounted_rewards-mean_rewards)/(std_rewards+1e-7)  \r\n",
        "\r\n",
        "    return norm_discounted_rewards\r\n",
        "\r\n",
        "  def update_policy(self):\r\n",
        "    states = np.asarray(self.states)\r\n",
        "    gradients=np.vstack(self.gradients)\r\n",
        "    rewards=np.vstack(self.rewards)\r\n",
        "    discounted_rewards=self.get_discounted_rewards(rewards)\r\n",
        "    \r\n",
        "    gradients*=discounted_rewards\r\n",
        "    #Calculate Gradients\r\n",
        "    gradients=self.alpha*np.vstack([gradients])+self.probs\r\n",
        "    \r\n",
        "    history=self.model.train_on_batch(states, gradients)\r\n",
        "    self.states, self.probs, self.gradients, self.rewards=[], [], [], []\r\n",
        "    return history\r\n",
        "\r\n",
        "  def validation_F1(self,X_test,Y_test):\r\n",
        "    Y_pred = []\r\n",
        "    for i in range(X_test.shape[0]):\r\n",
        "      action,_ = self.get_action(X_test[i])\r\n",
        "      Y_pred.append(action)\r\n",
        "    print(np.unique(Y_pred,return_counts=True))\r\n",
        "    f1 = f1_score(Y_test,Y_pred)\r\n",
        "    self.f1.append(f1)\r\n",
        "    return f1\r\n",
        "\r\n",
        "  def train(self,episodes,X_test,Y_test,rollout_n=1):\r\n",
        "    #Create the enviroment \r\n",
        "    env=self.env \r\n",
        "    total_rewards=np.zeros(episodes)\r\n",
        "    total_reward = 0\r\n",
        "    \r\n",
        "    #Run for Maximum number of episodes. \r\n",
        "    for episode in tq.tqdm(range(episodes)):\r\n",
        "      #Get the current state. \r\n",
        "      state = env.reset()\r\n",
        "      done=False          \r\n",
        "      episode_reward=0 \r\n",
        "      length = 0\r\n",
        "\r\n",
        "      while not done:\r\n",
        "        #Get the action.\r\n",
        "        action, prob = self.get_action(state)\r\n",
        "        #Get next state & reward.\r\n",
        "        next_state, reward, done = env.step(state,action)\r\n",
        "        \r\n",
        "        #Update the memory. \r\n",
        "        self.update_memory(state, action, prob, reward)\r\n",
        "        \r\n",
        "        #Update the current state. \r\n",
        "        state=next_state\r\n",
        "        episode_reward+=reward\r\n",
        "        length+=1\r\n",
        "        \r\n",
        "        #Once episode is over. \r\n",
        "        if done:\r\n",
        "          total_reward+=episode_reward\r\n",
        "\r\n",
        "          if episode%rollout_n==0:\r\n",
        "            #Update the policy and save the model.\r\n",
        "            history=self.update_policy()\r\n",
        "            #self.save_model()\r\n",
        "            if episode%15 == 0:\r\n",
        "              print('Episode :',episode,'Reward: ',total_reward,'Length: ',length,'F1 Score: ',self.validation_F1(X_test,Y_test))\r\n",
        "            break\r\n",
        "      total_rewards[episode]=episode_reward\r\n",
        "      self.total_rewards=total_rewards\r\n",
        "\r\n",
        "  def save_model(self):\r\n",
        "    self.model.save('/content/drive/MyDrive/RL Project/Vineet/REINFORCE_model_79_40per.h5')\r\n",
        "  \r\n",
        "  def load_model(self, path):\r\n",
        "    return load_model(path)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYdhY3VlOb57"
      },
      "source": [
        "REINFORCE = REINFORCE(Environment(X_train,Y_train,0.4),gamma,alpha,learning_rate)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHpHd9MTOqT5"
      },
      "source": [
        "REINFORCE.train(500,X_Val,Y_Val,1)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uAzZyxVSPrL"
      },
      "source": [
        "## Get Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fU74ijAfK-H3"
      },
      "source": [
        "Y_pred = []\r\n",
        "for i in range(X_val.shape[0]):\r\n",
        "  a,_ = REINFORCE.get_action((X_val[i].reshape(1,28,28,1)))\r\n",
        "  Y_pred.append(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsWB9akFMpoV"
      },
      "source": [
        "print('F1 Score: ',f1_score(Y_val,Y_pred))\r\n",
        "print('Accuracy: ',accuracy_score(Y_val,Y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}